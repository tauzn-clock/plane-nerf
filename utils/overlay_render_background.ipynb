{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/tinycudann/modules.py:31: UserWarning: System has multiple GPUs with different compute capabilities: [61, 52]. Using compute capability 52 for best compatibility. This may result in suboptimal performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from nerfstudio.utils.eval_utils import eval_setup\n",
    "from plane_nerf.inerf_utils import transform_original_space_to_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tiny-cuda-nn warning: FullyFusedMLP is not supported for the selected architecture 52. Falling back to CutlassMLP. For maximum performance, raise the target GPU architecture to 75+.\n",
      "tiny-cuda-nn warning: FullyFusedMLP is not supported for the selected architecture 52. Falling back to CutlassMLP. For maximum performance, raise the target GPU architecture to 75+.\n",
      "tiny-cuda-nn warning: FullyFusedMLP is not supported for the selected architecture 52. Falling back to CutlassMLP. For maximum performance, raise the target GPU architecture to 75+.\n",
      "tiny-cuda-nn warning: FullyFusedMLP is not supported for the selected architecture 52. Falling back to CutlassMLP. For maximum performance, raise the target GPU architecture to 75+.\n",
      "tiny-cuda-nn warning: FullyFusedMLP is not supported for the selected architecture 52. Falling back to CutlassMLP. For maximum performance, raise the target GPU architecture to 75+.\n",
      "tiny-cuda-nn warning: FullyFusedMLP is not supported for the selected architecture 52. Falling back to CutlassMLP. For maximum performance, raise the target GPU architecture to 75+.\n",
      "tiny-cuda-nn warning: FullyFusedMLP is not supported for the selected architecture 52. Falling back to CutlassMLP. For maximum performance, raise the target GPU architecture to 75+.\n",
      "tiny-cuda-nn warning: FullyFusedMLP is not supported for the selected architecture 52. Falling back to CutlassMLP. For maximum performance, raise the target GPU architecture to 75+.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading latest checkpoint from load_dir\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading latest checkpoint from load_dir\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Done loading checkpoint from \n",
       "outputs/jackal_floor_training_data_1/plane-nerf/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">03</span>-11_145657/nerfstudio_models/step-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000009999.</span>ckpt\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Done loading checkpoint from \n",
       "outputs/jackal_floor_training_data_1/plane-nerf/\u001b[1;36m2024\u001b[0m-\u001b[1;36m03\u001b[0m-11_145657/nerfstudio_models/step-\u001b[1;36m000009999.\u001b[0mckpt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.chdir('/workspace/plane-nerf')\n",
    "MODEL_PATH = \"/workspace/plane-nerf/outputs/jackal_floor_training_data_1/plane-nerf/2024-03-11_145657\"\n",
    "DATA_PATH = \"/workspace/plane-nerf/data/jackal_floor_evaluation_data\"\n",
    "BACKGROUND_IMG = \"/workspace/plane-nerf/data/jackal_floor_training_data_1/background.png\"\n",
    "config_path = os.path.join(MODEL_PATH, \"config.yml\")\n",
    "config, pipeline, _, _ = eval_setup(\n",
    "                        Path(config_path),\n",
    "                        test_mode=\"inference\",\n",
    "                    )\n",
    "transform_file_path = \"transforms.json\"\n",
    "with open(os.path.join(DATA_PATH, transform_file_path)) as f:\n",
    "    transform = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open background_img\n",
    "background_img = cv2.imread(BACKGROUND_IMG)\n",
    "background_img = cv2.cvtColor(background_img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Setting up training dataset<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Setting up training dataset\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Caching all <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span> images.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Caching all \u001b[1;36m300\u001b[0m images.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline.eval()\n",
    "pipeline.datamanager.setup_train()\n",
    "\n",
    "for camera, batch in pipeline.datamanager.fixed_indices_train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import structural_similarity_index_measure\n",
    "from torchmetrics.image import PeakSignalNoiseRatio\n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "\n",
    "psnr = PeakSignalNoiseRatio(data_range=1.0).to(pipeline.device)\n",
    "ssim = structural_similarity_index_measure\n",
    "lpips = LearnedPerceptualImagePatchSimilarity(normalize=True).to(pipeline.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:70: FutureWarning: Importing `spectral_angle_mapper` from `torchmetrics.functional` was deprecated and will be removed in 2.0. Import `spectral_angle_mapper` from `torchmetrics.image` instead.\n",
      "  _future_warning(\n"
     ]
    }
   ],
   "source": [
    "store_results = []\n",
    "\n",
    "for frame in transform[\"frames\"]:\n",
    "    print(frame[\"file_path\"])\n",
    "    img = cv2.imread(os.path.join(DATA_PATH, frame[\"file_path\"]))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = torch.tensor(img, dtype=torch.float32).to(pipeline.device) / 255.0\n",
    "    \n",
    "    tf = frame[\"transform_matrix\"]\n",
    "\n",
    "    tf = torch.tensor([tf[:3][:4]], dtype=torch.float32)\n",
    "    tf = transform_original_space_to_pose(tf, \n",
    "                                          pipeline.datamanager.train_dataparser_outputs.dataparser_transform,\n",
    "                                          pipeline.datamanager.train_dataparser_outputs.dataparser_scale,\n",
    "                                          \"opengl\")\n",
    "    camera.camera_to_worlds = tf.to(pipeline.device)\n",
    "    outputs = pipeline.model.get_outputs_for_camera(camera=camera)   \n",
    "\n",
    "    rendered_img = outputs[\"rgb\"]\n",
    "    \n",
    "    mask = cv2.imread(os.path.join(DATA_PATH, frame[\"mask_path\"]))\n",
    "\n",
    "    #Open mask as binary\n",
    "    mask = cv2.imread(os.path.join(DATA_PATH, frame[\"mask_path\"]), cv2.IMREAD_GRAYSCALE)\n",
    "    mask = mask > 0\n",
    "\n",
    "    mask = torch.tensor(mask, dtype=torch.float32).to(pipeline.device)\n",
    "    mask = torch.stack([mask, mask, mask], dim=-1)\n",
    "\n",
    "    masked_img = rendered_img * mask\n",
    "\n",
    "    #Add background\n",
    "    background = torch.tensor(background_img, dtype=torch.float32).to(pipeline.device) / 255.0\n",
    "    masked_img_with_background = masked_img + (1 - mask) * background\n",
    "\n",
    "    psnr_full = psnr(img.permute(2,0,1).unsqueeze(0),masked_img_with_background.permute(2,0,1).unsqueeze(0))\n",
    "    ssim_full = ssim(img.permute(2,0,1).unsqueeze(0),masked_img_with_background.permute(2,0,1).unsqueeze(0))\n",
    "    lpips_full = lpips(img.permute(2,0,1).unsqueeze(0),masked_img_with_background.permute(2,0,1).unsqueeze(0))\n",
    "\n",
    "    psnr_masked = psnr(img.permute(2,0,1).unsqueeze(0),masked_img.permute(2,0,1).unsqueeze(0))\n",
    "    ssim_masked = ssim(img.permute(2,0,1).unsqueeze(0),masked_img.permute(2,0,1).unsqueeze(0))\n",
    "    lpips_masked = lpips(img.permute(2,0,1).unsqueeze(0),masked_img.permute(2,0,1).unsqueeze(0))\n",
    "\n",
    "    store_results.append([float(psnr_full), float(ssim_full), float(lpips_full), float(psnr_masked), float(ssim_masked), float(lpips_masked)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.62395821e+01 9.86207664e-01 7.78639037e-03 1.50880861e+01\n",
      " 2.54929587e-02 8.55521262e-01]\n"
     ]
    }
   ],
   "source": [
    "#Save results as metrics.csv under DATA_PATH\n",
    "store_results = np.array(store_results)\n",
    "np.savetxt(os.path.join(MODEL_PATH, \"metrics.csv\"), store_results, delimiter=\",\")\n",
    "print(np.mean(store_results, axis=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
